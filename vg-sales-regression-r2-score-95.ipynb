{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Importing the libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Importing the dataset\ndataset = pd.read_csv('../input/videogamesales/vgsales.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for null values in the dataset\ndataset.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking which columns contain null values\nprint(dataset['Rank'].isnull().values.any())\nprint(dataset['Name'].isnull().values.any())\nprint(dataset['Platform'].isnull().values.any())\nprint(dataset['Year'].isnull().values.any())\nprint(dataset['Genre'].isnull().values.any())\nprint(dataset['Publisher'].isnull().values.any())\nprint(dataset['NA_Sales'].isnull().values.any())\nprint(dataset['EU_Sales'].isnull().values.any())\nprint(dataset['JP_Sales'].isnull().values.any())\nprint(dataset['Other_Sales'].isnull().values.any())\nprint(dataset['Global_Sales'].isnull().values.any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the number of missing value rows in the dataset\nprint(dataset['Year'].isnull().sum())\nprint(dataset['Publisher'].isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing the missing value rows in the dataset\ndataset = dataset.dropna(axis=0, subset=['Year','Publisher'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Defining the features and the dependent variable\nx = dataset.iloc[:,1:-1].values\ny = dataset.iloc[:,-1].values\nprint(x[0])\nprint(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Determining the relevancy of features using heatmap in calculating the outcome variable\ncorrmat = dataset.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(10,10))\n#Plotting heat map\ng=sns.heatmap(dataset[top_corr_features].corr(),annot=True,linewidths=.5)\nb, t = plt.ylim() # Finding the values for bottom and top\nb += 0.5 \nt -= 0.5 \nplt.ylim(b, t) \nplt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retaining only the useful features of the dataset\n# From the heatmap, we can decipher that the columns NA_Sales,JP_Sales,EU_Sales and Other_Sales are the most useful features\n# in determining the global sales\nx = dataset.iloc[:,6:-1].values\nprint(x[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Splitting the dataset into independent and dependent vaiables\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train)\nprint(x_test)\nprint(y_train)\nprint(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### Multiple Linear Regression","metadata":{}},{"cell_type":"code","source":"## Training the multiple linear regression on the training set\nfrom sklearn.linear_model import LinearRegression\nregressor_MultiLinear = LinearRegression()\nregressor_MultiLinear.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Predicting test results\ny_pred = regressor_MultiLinear.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating r2 score\nfrom sklearn.metrics import r2_score\nr2_MultiLinear = r2_score(y_test,y_pred)\nprint(r2_MultiLinear)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"markdown","source":"### Polynomial Regression","metadata":{}},{"cell_type":"code","source":"## Finding out the optimal degree of polynomial regression\nfrom sklearn.preprocessing import PolynomialFeatures\nsns.set_style('darkgrid')\nscores_list = []\npRange = range(2,6)\nfor i in pRange :\n    poly_reg = PolynomialFeatures(degree=i)\n    x_poly = poly_reg.fit_transform(x_train)\n    poly_regressor = LinearRegression()\n    poly_regressor.fit(x_poly,y_train)\n    y_pred = poly_regressor.predict(poly_reg.fit_transform(x_test))\n    scores_list.append(r2_score(y_test,y_pred))\nplt.plot(pRange,scores_list,linewidth=2)\nplt.xlabel('Degree of polynomial')\nplt.ylabel('r2 score with varying degrees')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training the polynomial regression on the training model\npoly_reg = PolynomialFeatures(degree=2)\nx_poly = poly_reg.fit_transform(x_train)\npoly_regressor = LinearRegression()\npoly_regressor.fit(x_poly,y_train)\ny_pred = poly_regressor.predict(poly_reg.fit_transform(x_test))\nr2_poly = r2_score(y_test,y_pred)\nprint(r2_poly)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### K-Nearest Neighbors Regression","metadata":{}},{"cell_type":"code","source":"## Finding the optimal number of neighbors for KNN regression\nfrom sklearn.neighbors import KNeighborsRegressor\nknnRange = range(1,11,1)\nscores_list = []\nfor i in knnRange:\n    regressor_knn = KNeighborsRegressor(n_neighbors=i)\n    regressor_knn.fit(x_train,y_train)\n    y_pred = regressor_knn.predict(x_test)\n    scores_list.append(r2_score(y_test,y_pred))\nplt.plot(knnRange,scores_list,linewidth=2,color='green')\nplt.xticks(knnRange)\nplt.xlabel('No. of neighbors')\nplt.ylabel('r2 score of KNN')\nplt.show()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the KNN model on the training set\nregressor_knn = KNeighborsRegressor(n_neighbors=7)\nregressor_knn.fit(x_train,y_train)\ny_pred = regressor_knn.predict(x_test)\nr2_knn = r2_score(y_test,y_pred)\nprint(r2_knn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### Decision Tree Regression","metadata":{}},{"cell_type":"code","source":"# Training the Decision Tree regression on the training model\nfrom sklearn.tree import DecisionTreeRegressor\nregressor_Tree = DecisionTreeRegressor(random_state=0)\nregressor_Tree.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting test results\ny_pred = regressor_Tree.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating r2 score\nr2_tree = r2_score(y_test,y_pred)\nprint(r2_tree)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Regression","metadata":{}},{"cell_type":"code","source":"# Finding out the optimal number of trees for Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nforestRange=range(50,500,50)\nscores_list=[]\nfor i in forestRange: \n    regressor_Forest = RandomForestRegressor(n_estimators=i,random_state=0)\n    regressor_Forest.fit(x_train,y_train)\n    y_pred = regressor_Forest.predict(x_test)\n    scores_list.append(r2_score(y_test,y_pred))\nplt.plot(forestRange,scores_list,linewidth=2,color='maroon')\nplt.xticks(forestRange)\nplt.xlabel('No. of trees')\nplt.ylabel('r2 score of Random Forest Reg.')\nplt.show()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the Random Forest regression on the training model\nregressor_Forest = RandomForestRegressor(n_estimators=100,random_state=0)\nregressor_Forest.fit(x_train,y_train)\ny_pred = regressor_Forest.predict(x_test)\nr2_forest = r2_score(y_test,y_pred)\nprint(r2_forest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"## Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nx_train = sc_x.fit_transform(x_train)\nx_test = sc_x.transform(x_test)\nsc_y = StandardScaler()\ny_train = sc_y.fit_transform(np.reshape(y_train,(len(y_train),1)))\ny_test = sc_y.transform(np.reshape(y_test,(len(y_test),1)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train)\nprint(x_test)\nprint(y_test)\nprint(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear Support Vector Regression","metadata":{}},{"cell_type":"code","source":"## Training the Linear SVR model on the training set\nfrom sklearn.svm import SVR\nregressor_SVR = SVR(kernel='linear')\nregressor_SVR.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Predicting test results\ny_pred = regressor_SVR.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Calculating r2 score\nr2_linearSVR = r2_score(y_test,y_pred)\nprint(r2_linearSVR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### Non-linear Support Vector Regression","metadata":{}},{"cell_type":"code","source":"## Training the Non-linear SVR model on the training set\nfrom sklearn.svm import SVR\nregressor_NonLinearSVR = SVR(kernel='rbf')\nregressor_NonLinearSVR.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Predicting test results\ny_pred = regressor_NonLinearSVR.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Calculating r2 score\nr2_NonlinearSVR = r2_score(y_test,y_pred)\nprint(r2_NonlinearSVR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"## Applying XGBoost Regression model on the training set\nfrom xgboost import XGBRegressor\nregressor_xgb = XGBRegressor()\nregressor_xgb.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Predicting test results\ny_pred = regressor_xgb.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Calculating r2 score\nr2_xgb = r2_score(y_test,y_pred)\nprint(r2_xgb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"## Comparing the r2 scores of different models\nlabelList = ['Multiple Linear Reg.','Polynomial Reg.','K-NearestNeighbors','Decision Tree','Random Forest',\n             'Linear SVR','Non-Linear SVR','XGBoost Reg.']\nmylist = [r2_MultiLinear,r2_poly,r2_knn,r2_tree,r2_forest,r2_linearSVR,r2_NonlinearSVR,r2_xgb]\nfor i in range(0,len(mylist)):\n    mylist[i]=np.round(mylist[i]*100,decimals=3)\nprint(mylist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,8))\nax = sns.barplot(x=labelList,y=mylist)\nplt.yticks(np.arange(0, 101, step=10))\nplt.title('r2 score comparison among different regression models',fontweight='bold')\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate('{:.3f}%'.format(height), (x +0.25, y + height + 0.8))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}